:toc: macro
:toclevels: 2
:toc-title: Table of Contents

ifdef::env-github[]
:tip-caption: :bulb:
:note-caption: :point_right:
:important-caption: :loudspeaker:
:caution-caption: :rotating_light:
:warning-caption: :warning:
endif::[]

= Spring Cheat Sheet

toc::[]

== Thread Management
Enable virtual threads for Tomcat.
[source,yaml]
----
spring:
  threads:
    virtual:
      enabled: true
----

== Persistence And JPA

=== Local Docker Setup for PostgreSQL
* Basis for a docker-compose.yaml:
+
[source,yaml]
----
services:
  products-db:
    image: postgres:16.3
    container_name: foo-db
    restart: always
    environment:
      POSTGRES_USER: user
      POSTGRES_PASSWORD: password
      POSTGRES_DB: foo
    volumes:
      - foo-db:/var/lib/postgresql/data
    ports:
      - 5432:5432
    command: ["postgres", "-c", "log_statement=all"]

  test-service-adminer:
    container_name: test-service-adminer
    image: adminer
    restart: always
    ports:
      - 9090:8080

volumes:
  foo-db:
----

=== General Setup
* Global setup:
+
[source,yaml]
----
spring:
  jpa:
    database: postgresql
    database-platform: org.hibernate.dialect.PostgreSQLDialect
    properties:
      hibernate:
        dialect: org.hibernate.dialect.PostgreSQLDialect
    open-in-view: true
    hibernate:
      ddl-auto: none
----

* Profile specific configuration:
+
[source,yaml]
----
spring:
  datasource:
    url: jdbc:postgresql://localhost:5432/ecommerce
    username: user
    password: password
  liquibase:
    change-log: classpath:/db/changelog/db.changelog-master.yaml
  web:
    resources:
      add-mappings: true
----

=== Liquibase Setup

==== Changeset Definition

* Root Changeset:
+
[source,yaml]
----
databaseChangeLog:
  - includeAll:
      path: classpath:/db/changelog/changes/
      errorIfMissingOrEmpty: true
----

* Referenced changeset files (order is selected by filename descending):
+
[source,sql]
----
--liquibase formatted sql

--changeset author:1
CREATE TABLE IF NOT EXISTS price
(
    product_id INTEGER   NOT NULL,
    valid_from TIMESTAMP NOT NULL,
    valid_to   TIMESTAMP NOT NULL,
    currency   VARCHAR(255),
    value      FLOAT,
    PRIMARY KEY (product_id, valid_from, valid_to)
);
--rollback DROP TABLE price;
----

==== Spring Boot Centric Integration
* Required Dependency
** Only Spring Boot Integration (Version maintained by Spring Boot BOM):
+
[source,xml]
----
<dependency>
    <groupId>org.liquibase</groupId>
    <artifactId>liquibase-core</artifactId>
</dependency>
----

* Reference to the Changelog:
+
[source,yaml]
----
spring:
  liquibase:
    change-log: classpath:/db/changelog/db.changelog-master.yaml
----

==== Additional Setup For Maven Plugin
#TODO


=== Logging and Debugging

==== Additions to the Docker file/Docker compose file

* Enable statement logging:
+
[source,yaml]
----
    command: ["postgres", "-c", "log_statement=all"]
----

==== Application Properties Changes

[source,yaml]
----
spring:
  jpa:
    show-sql: true
    properties:
      hibernate:
        generate_statistics: true
logging:
  level:
    org.hibernate.SQL: WARN
    org.hibernate.type.descriptor.sql: TRACE
----

=== JPA Auditing and Versioning

==== Auditing
. Creating an AuditorAware Provider (_nota bene:_ Modify fetching of current principal to current need and security configuration)
+
[source,java]
----
@Slf4j
public class AuditorAwareProvider implements AuditorAware<String> {
    @Override
    public Optional<String> getCurrentAuditor() {

        var principal = SecurityContextHolder.getContext().getAuthentication().getPrincipal();
        if(principal instanceof User user) {
            return Optional.of(user.getUsername());
        }
        return Optional.empty();
    }
}
----

. Enabling auditing via configuration and exposing the bean implementing the SPI
+
[source,java]
----
@Configuration
@Slf4j
@EnableJpaAuditing(auditorAwareRef = "auditorAware")
public class AuditingConfiguration {
    @Bean
    public AuditorAware<String> auditorAware(){
        return new AuditorAwareProvider();
    }
}
----

. Setting up auditing on `Entity` level
+
[source,java]
----
@Table(name = "price")
@Getter
@Setter
@NoArgsConstructor
@Entity
@EntityListeners(AuditingEntityListener.class) // enable auditing for the entity
public class Price {

  @Id
  @Column(name = "product_id")
  private Integer productId;

  //...

  @CreatedDate
  @Column(name="created_at")
  private LocalDateTime createdAt;

  @CreatedBy
  @Column(name="created_by")
  private String createdBy;

  @LastModifiedDate
  @Column(name = "updated_at")
  private LocalDateTime updatedAt;

  @LastModifiedBy
  @Column(name = "updated_by")
  private String updatedBy;
}
----

==== Versioning via Envers

. Add Dependency
[source,xml]
+
----
<dependency>
    <groupId>org.hibernate</groupId>
    <artifactId>hibernate-envers</artifactId>
    <version>${envers.version}</version>
</dependency>
----

. Add suitable versioning tables for audited entities

. Add `@Audited` Annotation to Entity Classes or Attributes
+
Don't forget to annotate non primitive associations as well if required.

== Consuming REST APIs

=== Using Spring 3 HTTP Interfaces with WebClient (Feign-Alike)
. Add a dependency to Spring Reactive Web
+
Required as this approach is based on the Reactive Stack.
+
[source,xml]
+
----
<dependency>
  <groupId>org.springframework.boot</groupId>
  <artifactId>spring-boot-starter-webflux</artifactId>
</dependency>
----

. Create an Interface annotated with `@HttpExchange defining available endpoints
+
[source,java]
----
@HttpExchange(accept = APPLICATION_JSON_VALUE, contentType = APPLICATION_JSON_VALUE)
public interface FooClient {

    @PostExchange
    FooResponse doAComplicatedRequest(@RequestBody FooRequest request);

}
----

. Provide a `HttpClient` Bean
+
[source,java]
----
    @Bean
    HttpClient httpClient() {
        return HttpClient.create()
                .responseTimeout(properties.timeout());
    }
----

. Provide a `HttpServiceProxyFactory` Bean based on the HTTPClient
+
[source,java]
----
    @Bean
    HttpServiceProxyFactory httpServiceProxyFactory(HttpClient client){
        var webclient = WebClient.builder()
                .clientConnector(new ReactorClientHttpConnector(client))
                .baseUrl(properties.baseUrl())
                .defaultHeader(API_AUTHORIZATION_KEY, API_AUTHORIZATION_KEY)
                .defaultStatusHandler(HttpStatusCode::isError,
                        error -> error.bodyToMono(ErrorRecord.class)
                                .flatMap(body -> Mono.error(new ErrorRecord(body.error().message()))))
                .build();

        return HttpServiceProxyFactory.builder(WebClientAdapter.forClient(webclient))
                .blockTimeout(properties.timeout())
                .build();
----

. Use the `HttpServiceProxyFactory` Bean to create the client based on the Interface provided
+
[source,java]
----
    @Bean
    public FooClient fooClient(HttpServiceProxyFactory httpServiceProxyFactory) {
        return httpServiceProxyFactory.createClient(FooClient.class);
    }
----

==== Error Handling
Since WebClient is based on Spring's Reactive Stack, error handling needs some adaptions as well.

#TODO

[source,java]
----
    @Bean
    HttpServiceProxyFactory httpServiceProxyFactory(HttpClient client){
        var webclient = WebClient.builder()
                ...
                .defaultStatusHandler(HttpStatusCode::isError,
                        error -> error.bodyToMono(ErrorRecord.class)
                                .flatMap(body -> Mono.error(new ErrorRecord(body.error().message()))))
                ...
                .build();
----


==== Additional Resources
* https://www.baeldung.com/spring-6-http-interface[Baeldung HTTP Interface Tutorial]

=== Using Spring 3 HTTP Interfaces with RestClient (Feign-Alike)
. Alike Spring 3 HTTP Interfaces using the WebClient, create an Interface annotated with `@HttpExchange defining available endpoints
+
[source,java]
----
@HttpExchange(accept = APPLICATION_JSON_VALUE, contentType = APPLICATION_JSON_VALUE)
public interface FooClient {

    @PostExchange
    FooResponse doAComplicatedRequest(@RequestBody FooRequest request);

}
----

. Provide a `RESTClient` Bean
+
[source,java]
----
@Bean
RestClient RestClient() {
    return RestClient.create("https://www.foo-base-url/api/v1");
}
----

. Use the `RestClient` to create a Client
+
[source,java]
----
@Bean
RestClient RestClient(RestClient restClient) {
    HttpServiceProxyFactory factory = HttpServiceProxyFactory
            .builderFor(RestClientAdapter.create(client))
            .build();
    return factory.createClient(FooClient.class);
}
----

==== Additional Resources
* https://www.youtube.com/watch?v=UDNrJAvKc0k[DanVega Rest Client Tutorial]

== JSON Handling

=== Navigating JSON Objects via `JsonNode`

=== Creating Custom Deserializer

=== Testing JSON Handling Functionality
Annotate JSON related tests as
[source,java]
----
@JsonTest
public class FooDeserializerTest{
    //...
}
----

=== Additional Resources

== Spring Kafka

=== Local Infrastructure Setup

Setup is using AKHQ and Confluent's Confluent's Community Docker Compose files. The docker-compose file is based on

* https://github.com/confluentinc/cp-all-in-one[Confluent's Community Docker Compose files].
* https://akhq.io/docs/[AKHQ Quick Preview Docker Compose File]

[source, yaml]
----
services:
  zookeeper:
    image: confluentinc/cp-zookeeper:7.5.0
    hostname: zookeeper
    container_name: zookeeper
    ports:
      - "2181:2181"
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000

  broker:
    image: confluentinc/cp-kafka:7.5.0
    hostname: broker
    container_name: broker
    depends_on:
      - zookeeper
    ports:
      - "29092:29092"
      - "9092:9092"
      - "9101:9101"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:29092,PLAINTEXT_HOST://localhost:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_JMX_PORT: 9101
      KAFKA_JMX_HOSTNAME: localhost

  schema-registry:
    image: confluentinc/cp-schema-registry:7.5.0
    hostname: schema-registry
    container_name: schema-registry
    depends_on:
      - broker
    ports:
      - "8081:8081"
    environment:
      SCHEMA_REGISTRY_HOST_NAME: schema-registry
      SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: 'broker:29092'
      SCHEMA_REGISTRY_LISTENERS: http://0.0.0.0:8081

  akhq:
    # build:
    #   context: .
    image: tchiotludo/akhq
    restart: unless-stopped
    environment:
      AKHQ_CONFIGURATION: |
        akhq:
          connections:
            docker-kafka-server:
              properties:
                bootstrap.servers: "broker:29092"
              schema-registry:
                url: "http://schema-registry:8081"
              connect:
                - name: "connect"
                  url: "http://connect:8083"
    ports:
      - "8080:8080"
    links:
      - broker
      - schema-registry

  connect:
    image: cnfldemos/kafka-connect-datagen:0.6.2-7.5.0
    hostname: connect
    container_name: connect
    depends_on:
      - broker
      - schema-registry
    ports:
      - "8083:8083"
    environment:
      CONNECT_BOOTSTRAP_SERVERS: 'broker:29092'
      CONNECT_REST_ADVERTISED_HOST_NAME: connect
      CONNECT_GROUP_ID: compose-connect-group
      CONNECT_CONFIG_STORAGE_TOPIC: docker-connect-configs
      CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_OFFSET_FLUSH_INTERVAL_MS: 10000
      CONNECT_OFFSET_STORAGE_TOPIC: docker-connect-offsets
      CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_STATUS_STORAGE_TOPIC: docker-connect-status
      CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_KEY_CONVERTER: org.apache.kafka.connect.storage.StringConverter
      CONNECT_VALUE_CONVERTER: io.confluent.connect.avro.AvroConverter
      CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: http://schema-registry:8081
      CONNECT_PLUGIN_PATH: "/usr/share/java,/usr/share/confluent-hub-components"
      CONNECT_LOG4J_LOGGERS: org.apache.zookeeper=ERROR,org.I0Itec.zkclient=ERROR,org.reflections=ERROR

  ksqldb-server:
    image: confluentinc/cp-ksqldb-server:7.5.0
    hostname: ksqldb-server
    container_name: ksqldb-server
    depends_on:
      - broker
      - connect
    ports:
      - "8088:8088"
    environment:
      KSQL_CONFIG_DIR: "/etc/ksql"
      KSQL_BOOTSTRAP_SERVERS: "broker:29092"
      KSQL_HOST_NAME: ksqldb-server
      KSQL_LISTENERS: "http://0.0.0.0:8088"
      KSQL_CACHE_MAX_BYTES_BUFFERING: 0
      KSQL_KSQL_SCHEMA_REGISTRY_URL: "http://schema-registry:8081"
      KSQL_KSQL_CONNECT_URL: "http://connect:8083"
      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_REPLICATION_FACTOR: 1
      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: 'true'
      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: 'true'

  ksqldb-cli:
    image: confluentinc/cp-ksqldb-cli:7.5.0
    container_name: ksqldb-cli
    depends_on:
      - broker
      - connect
      - ksqldb-server
    entrypoint: /bin/sh
    tty: true

  ksql-datagen:
    image: confluentinc/ksqldb-examples:7.5.0
    hostname: ksql-datagen
    container_name: ksql-datagen
    depends_on:
      - ksqldb-server
      - broker
      - schema-registry
      - connect
    command: "bash -c 'echo Waiting for Kafka to be ready... && \
                       cub kafka-ready -b broker:29092 1 40 && \
                       echo Waiting for Confluent Schema Registry to be ready... && \
                       cub sr-ready schema-registry 8081 40 && \
                       echo Waiting a few seconds for topic creation to finish... && \
                       sleep 11 && \
                       tail -f /dev/null'"
    environment:
      KSQL_CONFIG_DIR: "/etc/ksql"
      STREAMS_BOOTSTRAP_SERVERS: broker:29092
      STREAMS_SCHEMA_REGISTRY_HOST: schema-registry
      STREAMS_SCHEMA_REGISTRY_PORT: 8081

  rest-proxy:
    image: confluentinc/cp-kafka-rest:7.5.0
    depends_on:
      - broker
      - schema-registry
    ports:
      - 8082:8082
    hostname: rest-proxy
    container_name: rest-proxy
    environment:
      KAFKA_REST_HOST_NAME: rest-proxy
      KAFKA_REST_BOOTSTRAP_SERVERS: 'broker:29092'
      KAFKA_REST_LISTENERS: "http://0.0.0.0:8082"
      KAFKA_REST_SCHEMA_REGISTRY_URL: 'http://schema-registry:8081'
----

=== Maven Dependencies

==== Avro-Based Code Generation

Build plugin, runs during compile step.

[source, xml]
----
    <build>
        <plugins>
            <plugin>
                <groupId>org.apache.avro</groupId>
                <artifactId>avro-maven-plugin</artifactId>
                <version>1.11.0</version>
                <executions>
                    <execution>
                        <id>avro</id>
                        <phase>generate-sources</phase>
                        <goals>
                            <goal>schema</goal>
                        </goals>
                        <configuration>
                            <sourceDirectory>${project.parent.basedir}/avro/</sourceDirectory>
                            <outputDirectory>${project.basedir}/src/main/java/</outputDirectory>
                            <stringType>String</stringType>
                        </configuration>
                    </execution>
                </executions>
            </plugin>
        </plugins>
    </build>
----

Kafka/Confluent Dependencies
[source,xml]
----
    <!-- Spring Kafka -->
    <dependency>
      <groupId>org.springframework.boot</groupId>
      <artifactId>spring-boot-starter</artifactId>
    </dependency>
    <dependency>
      <groupId>org.springframework.kafka</groupId>
      <artifactId>spring-kafka</artifactId>
    </dependency>

    <dependency>
      <groupId>org.apache.avro</groupId>
      <artifactId>avro</artifactId>
      <version>${apache-avro.version}</version>
    </dependency>
    <dependency>
      <groupId>io.confluent</groupId>
      <artifactId>kafka-schema-registry-client</artifactId>
      <version>${confluent-kafka-dependencies.version}</version>
    </dependency>
    <dependency>
      <groupId>io.confluent</groupId>
      <artifactId>kafka-avro-serializer</artifactId>
      <version>${confluent-kafka-dependencies.version}</version>
    </dependency>
    <dependency>
      <groupId>io.confluent</groupId>
      <artifactId>kafka-streams-avro-serde</artifactId>
      <version>${confluent-kafka-dependencies.version}</version>
      <exclusions>
        <exclusion>
          <groupId>org.slf4j</groupId>
          <artifactId>slf4j-log4j12</artifactId>
        </exclusion>
      </exclusions>
    </dependency>
----

=== Producer

==== Spring Boot Configuration
Minimal Configuration for producing Avro Serialized Messages using a schema registry:
[source,yaml]
----
spring:
  kafka:
    bootstrap-servers: "localhost:9092"
    template:
      default-topic: foo
    producer:
      client-id: foo-producer
      retries: 3
      properties:
        schema:
          registry:
            url: "http://localhost:8081"
        retry:
          backoff.ms: 250
      acks: all
      key-serializer: org.apache.kafka.common.serialization.StringSerializer
      value-serializer: io.confluent.kafka.serializers.KafkaAvroSerializer
----

=== Consumer

==== Spring Boot Configuration
Minimal Configuration for consuming Avro Serialized Messages using a schema registry:
[source,yaml]
----
spring:
  kafka:
    bootstrap-servers: "localhost:9092"
    template:
      default-topic: foo
    consumer:
      client-id: foo-consumer
      group-id: foo-consumers
      key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      value-deserializer: io.confluent.kafka.serializers.KafkaAvroDeserializer
      properties:
        specific.avro.reader: true
        schema:
          registry:
            url: "http://localhost:8081"
----

==== Additional Resources
* https://docs.spring.io/spring-boot/reference/messaging/kafka.html#messaging.kafka[Basic Spring Boot Application.properties Documentation]
* https://docs.spring.io/spring-boot/api/java/org/springframework/boot/autoconfigure/kafka/KafkaProperties.html[Spring Boot Javadoc List of Supported Configuration Properties]

== Task Scheduling using Spring Boot

[source,java]
----
@Configuration
@EnableAsync
@EnableScheduling
public class AppConfig {
}
----

=== Additional Resources
* https://docs.spring.io/spring-framework/reference/integration/scheduling.html[Spring Documentation for Scheduling]
* https://medium.com/hprog99/mastering-job-scheduling-in-spring-boot-from-basics-to-best-practices-74ab938d80fa[Medium: Job Scheduling Best Practices]

== Maven

=== Multimodule Projects

Go through parent folders:
[source, xml]
----
<configuration>
    <sourceDirectory>${project.parent.basedir}/avro/</sourceDirectory>
    <outputDirectory>${project.parent.basedir}/src/main/java/</outputDirectory>
    <stringType>String</stringType>
</configuration>
----

== Other Stuff

=== "Load Testing" Local Applications

Via Apache Bench (`ab`):
[source,bash]
----
ab -c 20 //<1>
   -n 10 //<2>
  localhost:8080/foo/bar //<3>
----
<1> number of concurrent threads (will compete with threads of your application of course if run locally)
<2> number of requests
<3> URL

==== Additional Resources

* https://httpd.apache.org/docs/2.4/programs/ab.html[Documentation]